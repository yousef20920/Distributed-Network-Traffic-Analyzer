version: "3.8"

services:
  # Message broker (Redpanda - Kafka-compatible)
  redpanda:
    image: redpandadata/redpanda:v24.1.1
    container_name: redpanda
    command:
      - redpanda
      - start
      - --smp=1
      - --memory=1G
      - --reserve-memory=0M
      - --overprovisioned
      - --node-id=0
      - --kafka-addr=PLAINTEXT://0.0.0.0:9092
      - --advertise-kafka-addr=PLAINTEXT://redpanda:9092
    ports:
      - "9092:9092"
      - "9644:9644"
    volumes:
      - redpanda_data:/var/lib/redpanda/data
    healthcheck:
      test: ["CMD", "rpk", "cluster", "health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Create Kafka topic
  init-kafka:
    image: redpandadata/redpanda:v24.1.1
    container_name: init-kafka
    depends_on:
      redpanda:
        condition: service_healthy
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        rpk topic create netflow.raw --brokers redpanda:9092 -p 6 -r 1 || true
        rpk topic create netflow.alerts --brokers redpanda:9092 -p 3 -r 1 || true
        echo "Topics created successfully"

  # Spark Master
  spark-master:
    image: apache/spark:3.5.0-python3
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_NO_DAEMONIZE=true
    command: /opt/spark/sbin/start-master.sh
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ../spark-pyspark:/app
      - ../data:/data

  # Spark Worker 1
  spark-worker-1:
    image: apache/spark:3.5.0-python3
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_NO_DAEMONIZE=true
    command: /opt/spark/sbin/start-worker.sh spark://spark-master:7077
    depends_on:
      - spark-master
    volumes:
      - ../spark-pyspark:/app
      - ../data:/data

  # Spark Worker 2
  spark-worker-2:
    image: apache/spark:3.5.0-python3
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_NO_DAEMONIZE=true
    command: /opt/spark/sbin/start-worker.sh spark://spark-master:7077
    depends_on:
      - spark-master
    volumes:
      - ../spark-pyspark:/app
      - ../data:/data

  # Spark Streaming Job
  spark-streaming:
    image: apache/spark:3.5.0-python3
    container_name: spark-streaming
    user: root
    environment:
      - KAFKA_BROKERS=redpanda:9092
      - KAFKA_TOPIC=netflow.raw
      - BRONZE_PATH=/data/bronze/netflow
      - CHECKPOINT_PATH=/data/checkpoints/bronze
      - SILVER_PATH=/data/silver/netflow_enriched
      - SILVER_CHECKPOINT=/data/checkpoints/silver
      - GOLD_PATH=/data/gold
    command: >
      /opt/spark/bin/spark-submit
      --master spark://spark-master:7077
      --deploy-mode client
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
      --conf spark.executor.memory=1g
      --conf spark.driver.memory=1g
      --conf spark.sql.shuffle.partitions=6
      /app/src/streaming_job.py
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
      - redpanda
    volumes:
      - ../spark-pyspark:/app
      - ../data:/data
      - ivy_cache:/root/.ivy2
    restart: unless-stopped

  # Go Producer
  producer:
    build:
      context: ../producer-go
      dockerfile: ../docker/producer/Dockerfile
    container_name: producer
    environment:
      - KAFKA_BROKERS=redpanda:9092
      - TOPIC=netflow.raw
      - EVENTS_PER_SEC=1000
      - ROUTER_ID=router-1
      - SCENARIO=ddos_fan_in
    depends_on:
      init-kafka:
        condition: service_completed_successfully
    restart: unless-stopped

  # Streamlit Dashboard
  dashboard:
    build:
      context: ../dashboard
      dockerfile: ../docker/dashboard/Dockerfile
    container_name: dashboard
    environment:
      - DATA_DIR=/data
      - REFRESH_INTERVAL=5
    ports:
      - "8501:8501"
    volumes:
      - ../data:/data
    depends_on:
      - spark-master

volumes:
  redpanda_data:
  ivy_cache:
